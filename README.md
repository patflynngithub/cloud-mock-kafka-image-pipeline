

# Amazon Cloud Mock Image Pipeline Using Apache Kafka



## ## *NOTE: this project is just a proof-of-concept image processing pipeline that runs on the Amazon Cloud and utilizes Amazon Cloud's RDS MySQL relational database and S3 object database services. This project was created very quickly in order to get something that worked and so that I could move on to other tasks that would further prepare me for my first Caltech IPAC job interview. Its code is, in a few places, clean,  well-structured, well-organized and well-architected, but mostly the code is not this. The error checking is very incomplete. Its configuration, both external of and internal to the code, is decidedly very manual (e.g., Amazon Cloud network and access settings in the code and its build and configuration commands/scripts). Please don't infer from this code about my coding skills, style, habits and software engineering skills. It will all be cleaned up on a future date.



### Description

This is a mock image processing pipeline using Apache Kafka. It runs on the Amazon Cloud, using this cloud's RDS MySQL relational database and S3 object database services. The pipeline has Kafka python clients doing the different stages of image processing work. It is adapted from the original local Linux PC-based mock image pipeline that didn't use any Amazon Cloud resources.

You can look at a visualization of the pipeline in the *development_docs/* subdirectory.

The pipeline receives images, stores their metadata in a relational database and the images themselves in an object database, analyzes these images to detect image events of interest, stores the image event metadata in a relational database and the image event analysis-produced derivative images in an object database, and alerts subscribed parties about the image events, storing records of the sent alerts in a relational database.

There are also two standalone (non-Kafka) image event viewer options. The user enters an image event alert number to see the images associated with the image event. See the end of this document.

The mock aspects are:  

- The images are not received from an external source. They are generated by the Kafka image receiving client, starting with the original image of a computer-drawn cute teddy bear that is stored in the *image_original/* subdirectory. After several of the exact same "generated" images in the image stream, the most recently received image is changed by rotating it by 90 degrees and is used as the next image in the stream. This results in the Kafka image analysis client creating an image event of "image is different than the previous image."
- No astronomical image analysis techniques are used
- There is no image event subscription implementation
- No image event alerts are actually sent



#### Preliminaries

You will need to create your own Amazon Cloud EC2 computing instance, Amazon RDS MySQL database instance (you could, but you don't have to create a second Amazon EC2 compting instance for this), Amazon S3 object storage bucket, and various IAM security roles/credentials/settings and other security credentials/policies/settings. You can look at the python command-line scripts mentioned in the next paragraph for the host, database name, user, password and bucket name settings that I used. I cannot help much with the security stuff; it is all a blur now. I was very quickly searching for WWW suggestions and trying stuff until it finally worked. I definitely need to carefully read the Amazon Cloud security documentation and apply an enhanced understanding to methodically setting up the security for the mock image pipeline within a new, clean Amazon Cloud account.

The python command-line utilities that I created are in the *database_utility_scripts/* and *object_storage_utility_scripts/* subdirectories. They help with interacting with the Amazon Cloud relational and object databases. They can be run from inside or outside the Apache Kafka container.  Running them outside of the container on the EC2 computing instance's command-line may require you to install the Python MySQL connector (*pip install mysql-connector-python*) and the official Amazon Web Services (AWS) SDK for Python that enables developers to create, configure, and manage AWS services like S3, EC2, and DynamoDB (*pip install boto3*).  

The three Kafka python clients and *image_event_viewer_webpage/app.py* have host, database name, user, password and bucket name settings that are used to interact with the Amazon Cloud relational and object databases. If you used settings different from these, you will need to update their codes.

### Some of the following commands are required to be executed in the application's main directory where its core python (*.py) files are. You should *cd* to this directory before setting up and running all parts of the application



#### Accessing the Amazon EC2 computing instance

Before each of the setup/execution sections below, you will need a new ssh session into the Amazon EC2 computing instance. You do this from a local PC. For each ssh session, you will open up a new terminal window. This will allow you to view the separate text outputs of the Apache Kafka broker ("server") and its three pipeline clients when executing. You will need your own Amazon EC2 computing instance public/private key pair (public key stored in local *.pem* file; don't lose it!) and the instance's public DNS (or public IPv4 address). The below suggested command assumes that you have placed the mock image pipeline application (including all of its subirectories) in the instance's *~/cloud-mock-image-pipeline* subdirectory.

$ ssh -i /home/patrick/Desktop/holding/caltech/MockImagePipeline.pem ubuntu@ec2-35-94-18-229.us-west-2.compute.amazonaws.com -t "cd ~/cloud-mock-image-pipeline; exec $SHELL -i"

-----------------------------------------------------------------------

If this is the first time you are running the cloud mock image pipeline on your Amazon EC2 computing instance, you will need to create the Amazon RDS MySQL database using a python command-line utility that I created. This utility can be run from inside or outside the Apache Kafka container. Running it outside of the container on the EC2 computing instance's command-line may require you to install the Python MySQL connector (*pip install mysql-connector-python*). You may also need to change the host, user, and password settings inside the script.

Enter the following command:

$ python3 database_utility_scripts/create_pipeline_database.py

Be careful. If the database has already been created and has data in it, the database will be recreated and the data lost.

-----------------------------------------------------------------------

If this is not the first time you are running the cloud mock image pipeline on your Amazon EC2 computing instance, you may need to empty the Amazon Cloud relational and object databases. These utilities can be run from inside or outside the Apache Kafka container. Running them outside of the container on the EC2 computing instance's command-line may require you to install the Python MySQL connector (*pip install mysql-connector-python*) and the official Amazon Web Services (AWS) SDK for Python that enables developers to create, configure, and manage AWS services like S3, EC2, and DynamoDB (*pip install boto3*). You may need to change the host, user, password and bucket name settings inside the scripts.

Enter the following commands:

$ python3 database_utility_scripts/empty_pipeline_database.py

$ python3 object_storage_utility_scripts/empty_bucket.py

#### Build an Apache Kafka Docker image and create/run a Docker container that automatically executes Apache Kafka

Open a new command-line window and *ssh* to the EC2 computing instance. Enter the following commands:  

$ docker build -t pipeline_image .  
$ docker run --rm -v .:/pipeline --name pipeline_container -u="root" -p 9092:9092 pipeline_image

Note: that the *docker run* *--rm* argument causes the container to be automatically removed once it is exited via terminating Apache Kafka with a Ctrl-C.

#### Enter the Apache Kafka container and run the Kafka image event alert client

Open a new command-line window and *ssh* to the EC2 computing instance. Enter the following commands:  

$ docker exec -it pipeline_container /bin/bash  
     (you will automatically be put in the  /pipeline directory of the container)  
$ python3 image_event_alert.py

#### Enter the Apache Kafka container and run the Kafka image analysis client

Open a new command-line window and *ssh* to the EC2 computing instance. Enter the following commands:  

$ docker exec -it pipeline_container /bin/bash  
     (you will automatically be put in the  /pipeline directory of the container)  
$ python3 image_analysis.py

### Enter the Apache Kafka container and run the image receiving Kafka client
Open a new command-line window and *ssh* to the EC2 computing instance. Enter the following commands:  

$ docker exec -it pipeline_container /bin/bash  
     (you will automatically be put in the /pipeline directory of the container)  
$ python3 image_receiving.py



### If everything runs successfully (according to the text output of each client), you can further verify the contents of the application's Amazon Cloud RDS MySQL relational database and S3 object database bucket. This can be done with python command-line utilities that I created that can be run inside or outside of the Apache Kafka container (inside would be from one of the *docker exec* entries into the container). Running them outside of the container on the EC2 computing instance's command-line may require you to install the Python MySQL connector (*pip install mysql-connector-python*) and the official Amazon Web Services (AWS) SDK for Python, enabling developers to create, configure, and manage AWS services like S3, EC2, and DynamoDB (*pip install boto3*)

$ python3 database_utility_scripts/show_database_contents.py  

$ python3 object_storage_utility_scripts/show_bucket_contents.py  

### Image Event Viewer

An image event viewer allows a user to enter an image event alert number to view the images associated with the alert's image event.  

There are two alternative applications for this: 1) a Python Flask web server  (server-side python scripting) that runs inside of its own container. It is connected to by a web browser on your local PC, and 2) a standalone Python GUI that runs outside of the image pipeline container.  

The mock image pipeline is required to have completed successfully the last time the pipeline was run. This results in having the relational and object databases fully populated with image, image_event and image_event_alert data.

**Python Flask Web Server for Webpages**

Set up the Python Flask container that will run automatically run Flask's development web server. The following command, which builds the docker image and creates/runs the docker container, is in the *image_event_viewer_webpage/* subdirectory of the image pipeline main application directory.  

$ ./set_up_flask_container.sh  

To see the image event viewer webpage, in a web browser on your local PC enter into the web address field (security and related settings will have to have been properly set on the Amazon EC2 computing instance):  

http://35.94.18.229/  (this will be different for you; it will be the public IPv4 web address of your Amazon EC2 computing instance)

**Standalone Python GUI**  

*This program and its below documentation have not yet been updated from the non-cloud version of the mock image pipeline. It cannot be run with the cloud version of the mock image pipeline.*  

Requires access to the tkinter and Pillow python modules. The mock image pipeline and its container do not have to be currently running. From the image pipeline's main directory (where its *.py files are), run the following command:  

$ python3 image_event_viewer.py  

